# Model Configuration
# LLM and embedding model settings

# Primary LLM
llm:
  provider: "groq"  # Options: groq, openai, anthropic
  model: "llama-3.3-70b-versatile"
  api_key_env: "GROQ_API_KEY"
  
  # Generation parameters
  temperature: 0.2
  max_tokens: 4000
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0
  
  # Retry and timeout
  max_retries: 3
  timeout: 60
  
  # Rate limiting
  requests_per_minute: 30
  tokens_per_minute: 100000

# Embedding Model
embeddings:
  provider: "sentence-transformers"
  model: "sentence-transformers/all-MiniLM-L6-v2"
  batch_size: 32
  normalize: true
  
  # Caching
  cache_enabled: true
  cache_path: "data/embedding_cache"

# Groq Whisper STT (Speech-to-Text)
whisper:
  model: "whisper-large-v3"  # Options: whisper-large-v3, whisper-large-v3-turbo
  language: "en"

# Fallback models (if primary fails)
fallback:
  enabled: true
  models:
    - provider: "groq"
      model: "mixtral-8x7b-32768"
    - provider: "groq"
      model: "llama3-70b-8192"

# Model-specific overrides
overrides:
  # Use different settings for different agents
  router:
    temperature: 0.1
    max_tokens: 1000
  
  verifier:
    temperature: 0.1
    max_tokens: 2000
  
  explainer:
    temperature: 0.3
    max_tokens: 3000

# Cost tracking
cost_tracking:
  enabled: true
  log_path: "data/logs/cost.json"
  alert_threshold: 10.0  # Alert if cost exceeds $10
